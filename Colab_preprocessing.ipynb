{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ghZRo7m5ekrP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ]
    },
    {
      "metadata": {
        "id": "jv-gV4drer0G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip -q install unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bF5TjE6XlwUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip -q install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ul6hWGiFmDye",
        "colab_type": "code",
        "outputId": "387fcd90-cc72-4013-ca66-27af3fec25cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "! pip -q install  num2words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    10% |███▎                            | 10kB 21.1MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 20kB 3.4MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 30kB 4.9MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 40kB 3.1MB/s eta 0:00:01\r\u001b[K    51% |████████████████▌               | 51kB 3.8MB/s eta 0:00:01\r\u001b[K    62% |███████████████████▉            | 61kB 4.5MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▏        | 71kB 5.1MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 81kB 5.8MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▊  | 92kB 6.4MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 102kB 4.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6oiZcdfuhVmn",
        "colab_type": "code",
        "outputId": "82fd0536-32d1-4aef-9260-3f399b557600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "p91e-IQYekrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import unidecode\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tgtbeIQ2ekre",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sentence for examples"
      ]
    },
    {
      "metadata": {
        "id": "3V0EUct_ekri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_sentence = \"That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-KDG_gx0ekrp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "6U0c0Ow0ekrt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word tokenization"
      ]
    },
    {
      "metadata": {
        "id": "70bCT-Peekrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, WordPunctTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPMEuz3Lekr1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_text(text, word=True, tokenizer=None):\n",
        "    if tokenizer is None and word:\n",
        "        tokens = word_tokenize(text)\n",
        "    elif tokenizer is None and not word:\n",
        "        tokens = sent_tokenize(text)\n",
        "    else:\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "    return tokens "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KkHwP3cyekr6",
        "colab_type": "code",
        "outputId": "878f00e5-626c-479e-9025-0b3aef49f13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "# Original sentence\n",
        "print('Original:\\n {}'.format(example_sentence))\n",
        "\n",
        "# Tokenize a string to split off punctuation other than periods\n",
        "tokens = example_sentence.split()\n",
        "print('\\nWith a naive split:\\n {}'.format(tokens))\n",
        "\n",
        "# Tokenize a string to split off punctuation other than periods\n",
        "tokens = tokenize_text(example_sentence)\n",
        "print('\\nWith NLTK word tokenizer:\\n {}'.format(tokens))\n",
        "\n",
        "# Splits on punctuation, but keep it with the word\n",
        "tokens = tokenize_text(example_sentence, tokenizer=TreebankWordTokenizer())\n",
        "print('\\nWith TreebankWordTokenizer:\\n {}'.format(tokens))\n",
        "\n",
        "# Tokenize a text into a sequence of alphabetic and non-alphabetical characters using\n",
        "# the regex \\w+|[^\\w\\s]+\n",
        "tokens = tokenize_text(example_sentence, tokenizer=WordPunctTokenizer())\n",
        "print('\\nWith NLTK WordPunctTokenizer:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\n",
            "\n",
            "With a naive split:\n",
            " [\"That's\", 'àn', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n",
            "\n",
            "With NLTK word tokenizer:\n",
            " ['That', \"'s\", 'àn', 'example', ':', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%', '&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n",
            "\n",
            "With TreebankWordTokenizer:\n",
            " ['That', \"'s\", 'àn', 'example', ':', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%', '&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n",
            "\n",
            "With NLTK WordPunctTokenizer:\n",
            " ['That', \"'\", 's', 'àn', 'example', ':', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't', '¨¨', 'ïtled', 'Intelligence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iELOmfCJeksG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sentence tokenization"
      ]
    },
    {
      "metadata": {
        "id": "hIk1i1ineksN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YrlChHuQeksX",
        "colab_type": "code",
        "outputId": "32f39cde-078e-4691-a0fa-90cba6a2e47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "# Original sentence\n",
        "print('Original:\\n {}'.format(example_sentence))\n",
        "\n",
        "# Tokenize a text into a list of sequences by using NLTK's recommended sentence tokenizer (currently\n",
        "# PunktSentenceTokenizer)\n",
        "tokens = tokenize_text(example_sentence, word=False)\n",
        "print('\\nWith NLTK sentence tokenizer:\\n {}'.format(tokens))\n",
        "\n",
        "# Tokenize a text into a list of sequences by using an unsupervised algorithm to \n",
        "# build a model for abbreviation words, collocations, and words that start sentences\n",
        "tokens = tokenize_text(example_sentence, word=False, tokenizer=PunktSentenceTokenizer())\n",
        "print('\\nWith NLTK PunktSentenceTokenizer:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\n",
            "\n",
            "With NLTK sentence tokenizer:\n",
            " [\"That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\"]\n",
            "\n",
            "With NLTK PunktSentenceTokenizer:\n",
            " [\"That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vW_3BF8zekss",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Normalization"
      ]
    },
    {
      "metadata": {
        "id": "HVBxO5wDeksv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert characters to lower or upper case"
      ]
    },
    {
      "metadata": {
        "id": "Lh1gT0F3eksx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_characters(tokens, style='lower'):\n",
        "    if style == 'lower':\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "    else:\n",
        "        tokens = [token.upper() for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZzsYg8Keeks4",
        "colab_type": "code",
        "outputId": "74216e99-fde1-418e-b483-e6f2502ed430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "cell_type": "code",
      "source": [
        "# Original sentence\n",
        "print('Original:\\n {}'.format(example_sentence))\n",
        "\n",
        "tokens = convert_characters(example_sentence.split())\n",
        "print('\\nTo lowercase:\\n {}'.format(tokens))\n",
        "\n",
        "tokens = convert_characters(example_sentence.split(), style='upper')\n",
        "print('\\nTo uppercase:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\n",
            "\n",
            "To lowercase:\n",
            " [\"that's\", 'àn', 'example:', 'in', '1950', ',', 'alan', 'turing', 'públished', 'an', '%&', '&', 'article', 't¨¨ïtled', 'intelligence']\n",
            "\n",
            "To uppercase:\n",
            " [\"THAT'S\", 'ÀN', 'EXAMPLE:', 'IN', '1950', ',', 'ALAN', 'TURING', 'PÚBLISHED', 'AN', '%&', '&', 'ARTICLE', 'T¨¨ÏTLED', 'INTELLIGENCE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "INxnYKXeeks_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Removing blanks "
      ]
    },
    {
      "metadata": {
        "id": "5eE279EBektB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_blanks(tokens):\n",
        "    return [token.strip() for token in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXAqCzeZektJ",
        "colab_type": "code",
        "outputId": "62addfb3-a5b7-46f2-bd10-2a9e4585d66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = remove_blanks(example_sentence.split())\n",
        "print('Original:\\n {}'.format(tokens))\n",
        "print('\\nRemoving blanks:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " [\"That's\", 'àn', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n",
            "\n",
            "Removing blanks:\n",
            " [\"That's\", 'àn', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kl_lsdt-ektR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Removing punctuation, diacritics, etc."
      ]
    },
    {
      "metadata": {
        "id": "wTubJER_ektU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_punctuation(sentence, keep_apostrophe=False):\n",
        "    return re.sub(r'[^a-zA-Z0-9]', r' ', sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DDgNbKhpektZ",
        "colab_type": "code",
        "outputId": "10faf9e3-ddd8-41a7-f53c-d18fb8fa0e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = remove_punctuation(example_sentence)\n",
        "print('Original:\\n {}'.format(example_sentence))\n",
        "print('\\nRemoving punctuation:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\n",
            "\n",
            "Removing punctuation:\n",
            " That s  n example  In 1950   Alan Turing P BLISHED an      article t   tled Intelligence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "8R-PuIm2ektl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_accents(tokens):\n",
        "    tokens = [unidecode.unidecode(token) for token in tokens]\n",
        "    return(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ut8DyzTGektu",
        "colab_type": "code",
        "outputId": "28de4d84-5669-4c70-e736-7769039a8c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = remove_accents(example_sentence.split())\n",
        "print('Original:\\n {}'.format(example_sentence.split()))\n",
        "print('\\nRemoving punctuation:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " [\"That's\", 'àn', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n",
            "\n",
            "Removing punctuation:\n",
            " [\"That's\", 'an', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PUBLISHED', 'an', '%&', '&', 'article', 't\"\"itled', 'Intelligence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LCGTjM0vekt6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expanding contractions "
      ]
    },
    {
      "metadata": {
        "id": "jgCIk_XNekt8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "english_contractions_mapping = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
        "    \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
        "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
        "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
        "    \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
        "    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
        "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
        "    \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
        "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
        "    \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
        "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
        "    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
        "    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
        "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
        "    \"this's\": \"this is\",\n",
        "    \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
        "    \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
        "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
        "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
        "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
        "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
        "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
        "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
        "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
        "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
        "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
        "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
        "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
        "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
        "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4kwsnMQsekuE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def expand_match(contraction): \n",
        "        match = contraction.group(0) \n",
        "        first_char = match[0] \n",
        "        expanded_contraction = english_contractions_mapping.get(match) if english_contractions_mapping.get(match) else english_contractions_mapping.get(match.lower())                        \n",
        "        expanded_contraction = first_char+expanded_contraction[1:] \n",
        "        return expanded_contraction \n",
        "    \n",
        "def expand_contractions(sentence, english_contractions_mapping):    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(english_contractions_mapping.keys())),\n",
        "                                      flags=re.IGNORECASE|re.DOTALL) \n",
        "    return contractions_pattern.sub(expand_match, sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "UNh2H8RPekuM",
        "colab_type": "code",
        "outputId": "6ec48f11-d0dd-4b4f-8491-eaa66451e231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "expanded = [expand_contractions(txt, english_contractions_mapping) \n",
        "            for txt in sent_tokenize(example_sentence)]     \n",
        "print ('Original:\\n {}'.format(example_sentence))\n",
        "print ('\\nAfter expanding contractions:\\n {}'.format(' '.join(expanded)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " That's àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\n",
            "\n",
            "After expanding contractions:\n",
            " That is àn example: In 1950 , Alan Turing PÚBLISHED an %& & article t¨¨ïtled Intelligence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0W5q98rRekuZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Removing Stopwords "
      ]
    },
    {
      "metadata": {
        "id": "-Ts2AWhkekua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2dhpP4bzekud",
        "colab_type": "code",
        "outputId": "da1d9138-c413-41b4-8e39-ea74cb072575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "english_nltk_stopwords = stopwords.words('english')\n",
        "english_nltk_stopwords[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "Rl0otfLAekui",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_stopwords(tokens, stopwords_list):\n",
        "    return [token for token in tokens if token not in stopwords_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XgPvTFtdeku0",
        "colab_type": "code",
        "outputId": "fd2df33e-6357-415c-98d6-c7cf3ca8b988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = remove_stopwords(example_sentence.split(), english_nltk_stopwords)\n",
        "print('Original:\\n {}'.format(example_sentence.split()))\n",
        "print('\\nRemoving stopwords:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " [\"That's\", 'àn', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n",
            "\n",
            "Removing stopwords:\n",
            " [\"That's\", 'àn', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F0___wY_eku8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Correcting words"
      ]
    },
    {
      "metadata": {
        "id": "lEG2C1Mzeku-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Comparing with a corpus "
      ]
    },
    {
      "metadata": {
        "id": "d8rmSZ89ekvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVBunNQEekvE",
        "colab_type": "code",
        "outputId": "e6ff90cd-db71-435c-9166-8ca5cbc6a9cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Corpus with 500 samples of English-language text\n",
        "word_list = brown.words()\n",
        "print(len(word_list))\n",
        "print(word_list[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1161192\n",
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ugjMskkyekvK",
        "colab_type": "code",
        "outputId": "cfea0115-5ff1-4984-b27e-d1884a0c71a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "brown_vocabulary = list(set(word_list))\n",
        "print(len(brown_vocabulary))\n",
        "print(brown_vocabulary[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56057\n",
            "['1803-1895', 'Toobin', 'mates', '7-1', 'operetta', 'Tareytown', 'observance', 'leather-hard', '1515', 'trembles']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVPjdmFfekvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def verify_word(word, corpus_vocabulary):\n",
        "    return word in corpus_vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S3mddcNjekvU",
        "colab_type": "code",
        "outputId": "ba6456a0-3b0b-454d-c9f8-8386a40c49bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print('Word \"house\" in the corpus?:\\n {}'.format(verify_word('house', brown_vocabulary)))\n",
        "print('Word \"houuuuuse\" in the corpus?:\\n {}'.format(verify_word('houuuuuse', brown_vocabulary)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word \"house\" in the corpus?:\n",
            " True\n",
            "Word \"houuuuuse\" in the corpus?:\n",
            " False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qXMWScxFekvX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using an algorithm / model"
      ]
    },
    {
      "metadata": {
        "id": "SMW3Qpv0ekvZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Spelling corrector (source: http://norvig.com/spell-correct.html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNEKrBw6lphA",
        "colab_type": "code",
        "outputId": "179d5587-d28c-4b91-d9ce-a4f937b23b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "! wget http://norvig.com/big.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-03 10:08:44--  http://norvig.com/big.txt\n",
            "Resolving norvig.com (norvig.com)... 66.96.146.129\n",
            "Connecting to norvig.com (norvig.com)|66.96.146.129|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6488666 (6.2M) [text/plain]\n",
            "Saving to: ‘big.txt’\n",
            "\n",
            "big.txt             100%[===================>]   6.19M   910KB/s    in 7.3s    \n",
            "\n",
            "2019-04-03 10:08:52 (871 KB/s) - ‘big.txt’ saved [6488666/6488666]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7UaoSDz4ekvd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VW6aVsLoekvq",
        "colab_type": "code",
        "outputId": "4d43e6b9-29c2-4c0f-e131-64967f173667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print('speling\\t   ->\\t', correction('speling'))\n",
        "print('houuuse\\t   ->\\t', correction('houuuse'))\n",
        "print('fial\\t   ->\\t', correction('fial'))\n",
        "print('misstkaes  ->\\t', correction(\"misstkaes\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "speling\t   ->\t spelling\n",
            "houuuse\t   ->\t house\n",
            "fial\t   ->\t final\n",
            "misstkaes  ->\t mistakes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYwuLRTnekvy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Converting number to words"
      ]
    },
    {
      "metadata": {
        "id": "Q81CBTUeekv6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import num2words as n2w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I5WUdACEekv8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def num_to_words(tokens, lang='en'):\n",
        "    return [n2w.num2words(int(token), lang=lang) if token.isdigit() else token for token in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TIKUs0b6ekwF",
        "colab_type": "code",
        "outputId": "eb9ef28f-bf52-42a2-e812-cf91a453c3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = num_to_words(example_sentence.split())\n",
        "print('Original:\\n {}'.format(example_sentence.split()))\n",
        "print('\\nRemoving punctuation:\\n {}'.format(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            " [\"That's\", 'àn', 'example:', 'In', '1950', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n",
            "\n",
            "Removing punctuation:\n",
            " [\"That's\", 'àn', 'example:', 'In', 'one thousand, nine hundred and fifty', ',', 'Alan', 'Turing', 'PÚBLISHED', 'an', '%&', '&', 'article', 't¨¨ïtled', 'Intelligence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sytq1IO7ekwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lexical Normalization"
      ]
    },
    {
      "metadata": {
        "id": "hpinhjjHekwM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_ok = ['study', 'studies', 'studying', 'studied']\n",
        "words_wrong = ['university', 'universal', 'universe']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YSCYUi0PekwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stemming "
      ]
    },
    {
      "metadata": {
        "id": "b1Tg8--vekwS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0B2101KwekwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stem_word(word, stemmer):\n",
        "    print('Stemmer: {}\\toriginal word: {}\\t\\tstem: {}'.format(stemmer[0], word, stemmer[1].stem(word)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PVWxSFcJekwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stemmers = [('Porter\\t', PorterStemmer()),\n",
        "            ('Snowball', SnowballStemmer(language='english')),\n",
        "            ('Lancaster', LancasterStemmer())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zMVjhlwuekwb",
        "colab_type": "code",
        "outputId": "fa74ba64-1cca-4fae-c66c-13284fd0861d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "for word in words_ok:\n",
        "    for stemmer in stemmers:\n",
        "        stem_word(word, stemmer)\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemmer: Porter\t\toriginal word: study\t\tstem: studi\n",
            "Stemmer: Snowball\toriginal word: study\t\tstem: studi\n",
            "Stemmer: Lancaster\toriginal word: study\t\tstem: study\n",
            "\n",
            "\n",
            "Stemmer: Porter\t\toriginal word: studies\t\tstem: studi\n",
            "Stemmer: Snowball\toriginal word: studies\t\tstem: studi\n",
            "Stemmer: Lancaster\toriginal word: studies\t\tstem: study\n",
            "\n",
            "\n",
            "Stemmer: Porter\t\toriginal word: studying\t\tstem: studi\n",
            "Stemmer: Snowball\toriginal word: studying\t\tstem: studi\n",
            "Stemmer: Lancaster\toriginal word: studying\t\tstem: study\n",
            "\n",
            "\n",
            "Stemmer: Porter\t\toriginal word: studied\t\tstem: studi\n",
            "Stemmer: Snowball\toriginal word: studied\t\tstem: studi\n",
            "Stemmer: Lancaster\toriginal word: studied\t\tstem: study\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "yMUF4sfGekwk",
        "colab_type": "code",
        "outputId": "591639f5-2a0e-41ea-a8d3-df79691c337d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "cell_type": "code",
      "source": [
        "for word in words_wrong:\n",
        "    for stemmer in stemmers:\n",
        "        stem_word(word, stemmer)\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemmer: Porter\t\toriginal word: university\t\tstem: univers\n",
            "Stemmer: Snowball\toriginal word: university\t\tstem: univers\n",
            "Stemmer: Lancaster\toriginal word: university\t\tstem: univers\n",
            "\n",
            "\n",
            "Stemmer: Porter\t\toriginal word: universal\t\tstem: univers\n",
            "Stemmer: Snowball\toriginal word: universal\t\tstem: univers\n",
            "Stemmer: Lancaster\toriginal word: universal\t\tstem: univers\n",
            "\n",
            "\n",
            "Stemmer: Porter\t\toriginal word: universe\t\tstem: univers\n",
            "Stemmer: Snowball\toriginal word: universe\t\tstem: univers\n",
            "Stemmer: Lancaster\toriginal word: universe\t\tstem: univers\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JRMhvVFbekwn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lemmatization "
      ]
    },
    {
      "metadata": {
        "id": "BZc-4n_Fekwp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "XbmSqHw0ekwr",
        "colab_type": "code",
        "outputId": "5618a6ac-a63c-45d2-9fdb-dd195418f87b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "for word in words_ok:\n",
        "    print('Lemmatizer: {}\\toriginal word: {}\\t\\tstem: {}'.format('WordNet', word, lemmatizer.lemmatize(word)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemmatizer: WordNet\toriginal word: study\t\tstem: study\n",
            "Lemmatizer: WordNet\toriginal word: studies\t\tstem: study\n",
            "Lemmatizer: WordNet\toriginal word: studying\t\tstem: studying\n",
            "Lemmatizer: WordNet\toriginal word: studied\t\tstem: studied\n",
            "Lemmatizer: WordNet\toriginal word: saw\t\tstem: saw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "skEvNPFzekwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ----"
      ]
    },
    {
      "metadata": {
        "id": "krImxfKvekwu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# To Do: Build an end-to-end preprocessing pipeline"
      ]
    },
    {
      "metadata": {
        "id": "NoDycHl3ekwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = \"Stemming isn't funier than attendign  to a MACHINE LEARNING  class. In  eaister I'm going to Andalucía . But, I can't stay there more "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DaBkpbTyekw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AV4fsugJekw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Wt6A2uNekw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZklCL93zekxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1Opj0x1ekxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uxq3-Hs7ekxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Ll8dwgrekxR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRuxTcjoekxY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VIPkzhX8ekxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDx49EaXekxd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Weqzcd-3ekxg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}